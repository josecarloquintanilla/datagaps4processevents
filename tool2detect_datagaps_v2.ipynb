{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92739ae4-2450-474e-92ac-b10b486a9702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: 'prod'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "env = os.environ.get('Environment')\n",
    "print(f\"Environment: {env!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e358e63a-59d2-409d-abaf-24a4ac34f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext jupyter_dmdg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a74b207-36ca-4dc5-a618-3f1d52d4046e",
   "metadata": {},
   "source": [
    "#### Part 1: Obtain a dataframe with the process events that a building of a site has in the Data Lake (DL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d743f54-1514-40ae-ba55-05d71e2e37dc",
   "metadata": {},
   "source": [
    "##### 1.1. Query process events from the DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0d77aa9-3a39-4174-81ab-ca3c550b7975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting these event variables is sufficient to obtain all process events\n",
    "# In addition, we select site_id and building_id because we will filter based on these two variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf571b88-08cb-4336-a062-184013b8cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%athena_to_df --out df_DL\n",
    "SELECT\n",
    "    site_id,\n",
    "    building_id,\n",
    "    event_name,\n",
    "    event_name_full AS recipe_full,\n",
    "    event_parameter_name AS parameter_name,  \n",
    "    event_parameter_value AS parameter_value,\n",
    "    event_start AS start_date,\n",
    "    event_stop AS end_date\n",
    "FROM \"prod_plant_connectivity_prepared_data\".\"process_events\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3008b63-4f7f-4f05-a247-cf737000bfd3",
   "metadata": {},
   "source": [
    "##### 1.2. Enter a specific site_id and a specific building_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1b1e10d-ee40-48ff-937d-80b2140623fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thus the resulting dataframe (df) only includes events for the site building entered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9591137e-5b6d-4312-b107-5e7ea16e1052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter site_id:  site_id\n",
      "Enter your site_id:  MLE\n",
      "Enter building_id:  building_id\n",
      "Enter your building_id:  0V10\n"
     ]
    }
   ],
   "source": [
    "column_name1 = input(\"Enter site_id: \")\n",
    "value1 = input(\"Enter your site_id: \")\n",
    "column_name2 = input(\"Enter building_id: \")\n",
    "value2 = input(\"Enter your building_id: \")\n",
    "df_DL_site_building = df_DL[(df_DL[column_name1] == value1) & (df_DL[column_name2] == value2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37913807-8c0a-4afe-9912-f75359e064c4",
   "metadata": {},
   "source": [
    "##### 1.3. Match the resulting df to the format of the csv containing the process events extracted directly from the sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be602173-ad13-4106-8766-cd1cb43b6cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are asked to indicate the time difference\n",
    "# This is because in the site the timestamp columns are usually filled with local time, while in DL UTC is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dc4fe9d-1625-4469-a7b0-48767af7d7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter time difference:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "141270"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_DL_site_building = df_DL_site_building.copy()\n",
    "time_difference = int(input(\"Enter time difference: \"))\n",
    "df_DL_site_building.loc[:, 'start_date'] += pd.Timedelta(hours=time_difference)\n",
    "df_DL_site_building.loc[:, 'end_date'] += pd.Timedelta(hours=time_difference)\n",
    "df_DL_site_building.loc[:, 'parameter_value'] = df_DL_site_building['parameter_value'].astype(float)\n",
    "df_DL_site_building = df_DL_site_building.astype(str)\n",
    "len(df_DL_site_building)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb6a2fe-f6d0-4300-b51d-4593a624d7d0",
   "metadata": {},
   "source": [
    "#### Part 2: Obtain a df with the process events extracted directly from the data sources (DS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbe4ffd-4890-4f1e-b74e-0467c29423e7",
   "metadata": {},
   "source": [
    "##### 2.1. Read the file coming from DS and take into account the deduplication of the process events in the DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e63c702-5872-4d7e-ab02-ab526d185cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Such deduplication means when two or more process events are identical except for the parameter_ts, \n",
    "# only one is kept in the DL and the rest are deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aa5d82e-76d0-4ada-82eb-53821b0f0e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramater_ts is the time when the info was available/saved into the historian\n",
    "# Its name depends on the site (for Marcy is DATE_PARAMETRE, e.g.); there are sites that do not use this column yet\n",
    "# That is why it is asked to specify the name of this variable (if it is not used yet, enter na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66083931-9e6c-4c52-ba52-865713c96684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your source csv file:  MLE/0V10/MLE_0V10.csv\n",
      "Enter the parameter_ts as it appears in the source:  DATE_PARAMETRE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5182"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_file_to_read = input(\"Enter your source csv file: \")\n",
    "df_all_process_events = pd.read_csv(source_file_to_read)\n",
    "df_all_process_events = df_all_process_events.reset_index().rename(columns={'index': 'ID'})\n",
    "df_all_process_events['ID'] = df_all_process_events['ID'].astype(str)\n",
    "sort_column = input(\"Enter the parameter_ts as it appears in the source: \")\n",
    "if sort_column != \"na\":\n",
    "    df_all_process_events.sort_values(by=sort_column, ascending=False, inplace=True)\n",
    "df_all_process_events.drop_duplicates(subset=['RECIPE_FULL', 'PARAMETER_NAME', 'PARAMETER_VALUE', 'START_DATE', 'END_DATE'], keep='first', inplace=True)\n",
    "len(df_all_process_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d78df6-bda1-4952-b92e-8e29f145f582",
   "metadata": {},
   "source": [
    "##### 2.2. Make sure that the process events come from mapped equipment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca1ed65c-4cd8-4074-a57a-3360030a8b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Streaming Engine's Confluence space you have a page that explains how to extract this info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac1447d-24eb-4008-935b-2ab756c1f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_equipment_file_to_read = input(\"Enter your mapped equipment file: \")\n",
    "df_mapped_equipment = pd.read_csv(mapped_equipment_file_to_read)\n",
    "df_DS = df_all_process_events[df_all_process_events['EQUIPMENT_NAME'].isin(df_mapped_equipment['EQUIPMENT_NAME'])]\n",
    "len(df_DS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f800e3d3-7b1a-4dad-b8cb-86200e2a737d",
   "metadata": {},
   "source": [
    "##### 2.3. Adapt the resulting df to the format of the df with the process events extracted from the DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c74718-b586-47f0-91c4-410e506716ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DS.columns = df_DS.columns.str.lower()\n",
    "df_DS = df_DS.astype(str)\n",
    "df_DS['start_date'] = df_DS['start_date'].str.slice(stop=-4)\n",
    "df_DS['end_date'] = df_DS['end_date'].str.slice(stop=-4)\n",
    "len(df_DS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6f9b94-4d3f-423b-a070-43e2893484a9",
   "metadata": {},
   "source": [
    "#### Part 3: Obtain a csv with datagaps only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbdde25-1826-40ea-8f21-f40c4d3936d0",
   "metadata": {},
   "source": [
    "##### 3.1. See if the process events extracted directly from the source are in the df extracted from the DL or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab91b2-d443-41d2-b6fa-ac5af3735033",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = pd.merge(df_DS, df_DL_site_building, on=['recipe_full', 'parameter_name', 'parameter_value', 'start_date', 'end_date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21463766-7d3a-45f8-bee2-a3ec3435df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only those process events that do not appear in the DL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a1695a-16d3-44ed-9b7f-7ec4f2f23ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagaps = df_merge[df_merge['event_name'].isna()]\n",
    "datagaps = datagaps.drop('event_name', axis=1)\n",
    "len(datagaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731ed3fe-d719-4e72-a651-0e8a95337ad5",
   "metadata": {},
   "source": [
    "##### 3.2. To have the df with datagaps in the same format as the input file with the source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f68c61-7a7f-4f41-913c-35280012e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_all_process_events['ID'].isin(datagaps['id'])\n",
    "result = df_all_process_events[mask]\n",
    "result = result.drop('ID', axis=1)\n",
    "len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849e405b-a0c9-4524-8b35-25a7aa478507",
   "metadata": {},
   "source": [
    "##### 3.3. Keep only datagaps whose end_date is greater than parameter_ts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a62ca4-c68f-4584-8d43-d1f5213230cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reason is that a process event is defined by a name, a start date and an end date\n",
    "# If the parameter_ts is not within that time interval, yes, there is datagap,\n",
    "# but the root cause is not Streaming Enigne but a lower level\n",
    "# This step only applies if the site uses the variable parameter_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6661f5f5-020a-4262-a95b-ed2eb5af6c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sort_column != \"na\":\n",
    "    filtered_result = result[result[sort_column] <= result['END_DATE']]\n",
    "else:\n",
    "    filtered_result = result\n",
    "len(filtered_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a5881e-145d-4132-bb48-7ec4a80448df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a csv with all the datagaps for the building of the site in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73a2ef7-323b-4980-bf35-92bf1a45d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'{value1}/{value2}/datagaps_{value1}_{value2}.csv'\n",
    "filtered_result.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a547e4-b105-4b20-be7e-a4d3caa9a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To represent the number of datagaps per equipment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fae5360-36ae-43a0-bbeb-c9b6d1ac32c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "var1 = filtered_result['EQUIPMENT_NAME'].value_counts()\n",
    "plt.figure(figsize=(10, 5))\n",
    "var1.plot(kind='bar')\n",
    "plt.xlabel('Equipment name')\n",
    "plt.ylabel('Number of datagaps')\n",
    "plt.title('Number of datagaps per equipment')\n",
    "fig_path = f'{value1}/{value2}/datagaps_{value1}_{value2}_per_equipment.png'\n",
    "plt.savefig(fig_path)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598fa1ae-2853-47d1-a0af-c51009a866cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To represent the number of datagaps vs the day they were stored in the historian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3322efcd-dc4a-4111-81fe-69007756d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_result = filtered_result.copy()\n",
    "filtered_result['DATE_PARAMETRE'] = pd.to_datetime(filtered_result['DATE_PARAMETRE'])\n",
    "filtered_result.set_index('DATE_PARAMETRE', inplace=True)\n",
    "weekly_counts = filtered_result.groupby(pd.Grouper(freq='D')).count()\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(weekly_counts.index, weekly_counts['BATCH_ID'])\n",
    "plt.xlabel('Date parameter day')\n",
    "plt.ylabel('Number of datagaps')\n",
    "plt.title('Number of datagaps per date parameter day')\n",
    "fig_path = f'{value1}/{value2}/datagaps_{value1}_{value2}_per_day.png'\n",
    "plt.savefig(fig_path)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f91ae7-67f0-4a6f-b0b5-39c5caa05763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b01c58-80cd-4a99-a5eb-d5185f35eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Considering total number of rows of the initial csv as total number of process events and only datagaps when date parameter is lower than end date: \" + str(round(len(filtered_result)*100/len(df_all_process_events),2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
